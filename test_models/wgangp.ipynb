{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\No\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def cond_gradient_penalty(critic, labels, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def save_checkpoint(state, filename=\"celeba_wgan_gp.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, gen, disc):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    gen.load_state_dict(checkpoint['gen'])\n",
    "    disc.load_state_dict(checkpoint['disc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Discriminator and Generator implementation from DCGAN paper\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class CondDiscriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d, num_classes, img_size):\n",
    "        super(CondDiscriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(channels_img + 1, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, img_size*img_size)\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embed(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim = 1) # NxCx H x W\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class CondGenerator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size):\n",
    "        super(CondGenerator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise + embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.embed = nn.Embedding(num_classes, embed_size)\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # latent vector z: N x noise_dim x 1 x 1\n",
    "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) # NxCx1x1\n",
    "        x = torch.cat([x, embedding], dim = 1)\n",
    "        return self.net(x)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 10/252                   Loss D: -16.8394, loss G: 11.5966\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 20/252                   Loss D: -35.3290, loss G: 21.5340\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 30/252                   Loss D: -48.8040, loss G: 27.7969\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 40/252                   Loss D: -58.0421, loss G: 32.4886\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 50/252                   Loss D: -68.0826, loss G: 37.6562\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 60/252                   Loss D: -79.1376, loss G: 44.9811\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 70/252                   Loss D: -82.7072, loss G: 48.0456\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 80/252                   Loss D: -95.8969, loss G: 54.0749\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 90/252                   Loss D: -94.4046, loss G: 57.3848\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 100/252                   Loss D: -100.1968, loss G: 59.6109\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 110/252                   Loss D: -111.9997, loss G: 66.3071\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 120/252                   Loss D: -117.0295, loss G: 70.2212\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 130/252                   Loss D: -122.4762, loss G: 76.8286\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 140/252                   Loss D: -116.3369, loss G: 75.1148\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 150/252                   Loss D: -126.1253, loss G: 80.0791\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 160/252                   Loss D: -122.7117, loss G: 84.4054\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 170/252                   Loss D: -127.7702, loss G: 88.6541\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 180/252                   Loss D: -118.2624, loss G: 87.2210\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 190/252                   Loss D: -129.9303, loss G: 95.6304\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 200/252                   Loss D: -129.1356, loss G: 96.8502\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 210/252                   Loss D: -130.7894, loss G: 101.2541\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 220/252                   Loss D: -139.5222, loss G: 105.7669\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 230/252                   Loss D: -123.3569, loss G: 105.3042\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 240/252                   Loss D: -126.5619, loss G: 107.6709\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [0/100] Batch 250/252                   Loss D: -124.0032, loss G: 107.3882\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 10/252                   Loss D: -134.0895, loss G: 119.0902\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 20/252                   Loss D: -117.9966, loss G: 113.6406\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 30/252                   Loss D: -140.3860, loss G: 126.0607\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 40/252                   Loss D: -120.6015, loss G: 116.7575\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 50/252                   Loss D: -113.4752, loss G: 121.2417\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 60/252                   Loss D: -130.4417, loss G: 129.5957\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 70/252                   Loss D: -121.3877, loss G: 129.6579\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 80/252                   Loss D: -126.5652, loss G: 135.9851\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 90/252                   Loss D: -114.9952, loss G: 131.4263\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 100/252                   Loss D: -113.8827, loss G: 131.1005\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 110/252                   Loss D: -103.5900, loss G: 124.7817\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 120/252                   Loss D: -117.5719, loss G: 143.0559\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 130/252                   Loss D: -113.1472, loss G: 136.6692\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 140/252                   Loss D: -110.3823, loss G: 132.7362\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 150/252                   Loss D: -96.9150, loss G: 123.8503\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 160/252                   Loss D: -102.4203, loss G: 142.5040\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 170/252                   Loss D: -106.6679, loss G: 137.0458\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 180/252                   Loss D: -101.0540, loss G: 133.6595\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 190/252                   Loss D: -92.3177, loss G: 121.7045\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 200/252                   Loss D: -92.1048, loss G: 129.2345\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 210/252                   Loss D: -94.5178, loss G: 130.9007\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 220/252                   Loss D: -78.8054, loss G: 127.7621\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 230/252                   Loss D: -78.9439, loss G: 122.0285\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 240/252                   Loss D: -85.3795, loss G: 127.3360\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [1/100] Batch 250/252                   Loss D: -84.6191, loss G: 123.4508\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 10/252                   Loss D: -84.0294, loss G: 118.2361\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 20/252                   Loss D: -74.7398, loss G: 116.0040\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 30/252                   Loss D: -83.2012, loss G: 129.1982\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 40/252                   Loss D: -67.7326, loss G: 119.2140\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 50/252                   Loss D: -75.2551, loss G: 121.2881\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 60/252                   Loss D: -74.9377, loss G: 119.9785\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 70/252                   Loss D: -64.5964, loss G: 117.5713\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 80/252                   Loss D: -64.7840, loss G: 116.2575\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 90/252                   Loss D: -58.6080, loss G: 115.7408\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 100/252                   Loss D: -61.9982, loss G: 115.1076\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 110/252                   Loss D: -49.5282, loss G: 117.4056\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 120/252                   Loss D: -51.5252, loss G: 113.6234\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 130/252                   Loss D: -48.9860, loss G: 112.1395\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 140/252                   Loss D: -44.9240, loss G: 116.4506\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 150/252                   Loss D: -41.1987, loss G: 108.8577\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 160/252                   Loss D: -45.7968, loss G: 109.9147\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 170/252                   Loss D: -48.0558, loss G: 114.3438\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 180/252                   Loss D: -36.2652, loss G: 112.1883\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 190/252                   Loss D: -41.6440, loss G: 110.8681\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 200/252                   Loss D: -44.9748, loss G: 112.4401\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 210/252                   Loss D: -42.9566, loss G: 110.9556\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 220/252                   Loss D: -40.3787, loss G: 109.3122\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 230/252                   Loss D: -36.7524, loss G: 113.7649\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 240/252                   Loss D: -30.7497, loss G: 112.5476\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/100] Batch 250/252                   Loss D: -32.6617, loss G: 112.7966\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 10/252                   Loss D: -29.3084, loss G: 113.7152\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 20/252                   Loss D: -26.2238, loss G: 112.5209\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 30/252                   Loss D: -22.3548, loss G: 115.8338\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 40/252                   Loss D: -26.9000, loss G: 114.1287\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 50/252                   Loss D: -29.0473, loss G: 114.4421\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 60/252                   Loss D: -21.5910, loss G: 118.7864\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 70/252                   Loss D: -22.0189, loss G: 115.4101\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 80/252                   Loss D: -23.5060, loss G: 117.2045\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 90/252                   Loss D: -19.6179, loss G: 117.5136\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 100/252                   Loss D: -18.6962, loss G: 116.9664\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 110/252                   Loss D: -20.7921, loss G: 115.2077\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 120/252                   Loss D: -16.0931, loss G: 118.3458\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 130/252                   Loss D: -20.4183, loss G: 115.2996\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 140/252                   Loss D: -16.9291, loss G: 118.7320\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 150/252                   Loss D: -20.5827, loss G: 117.9254\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 160/252                   Loss D: -16.9389, loss G: 121.3882\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 170/252                   Loss D: -17.7147, loss G: 122.5544\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 180/252                   Loss D: -16.7617, loss G: 121.7192\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 190/252                   Loss D: -15.1568, loss G: 118.9951\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 200/252                   Loss D: -14.1336, loss G: 119.3952\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 210/252                   Loss D: -17.7660, loss G: 118.8328\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 220/252                   Loss D: -15.2681, loss G: 121.3434\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 230/252                   Loss D: -10.7866, loss G: 124.1906\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 240/252                   Loss D: -16.5448, loss G: 122.8291\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/100] Batch 250/252                   Loss D: -12.4844, loss G: 118.3631\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 10/252                   Loss D: -16.0735, loss G: 120.1796\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 20/252                   Loss D: -12.4024, loss G: 124.0715\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 30/252                   Loss D: -12.7701, loss G: 122.8697\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 40/252                   Loss D: -9.4947, loss G: 121.7937\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 50/252                   Loss D: -10.6581, loss G: 127.8013\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 60/252                   Loss D: -11.4234, loss G: 117.4162\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 70/252                   Loss D: -13.3356, loss G: 125.6048\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 80/252                   Loss D: -10.1343, loss G: 118.0806\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 90/252                   Loss D: -12.0046, loss G: 120.7026\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 100/252                   Loss D: -9.8150, loss G: 121.1290\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 110/252                   Loss D: -9.3441, loss G: 116.9320\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 120/252                   Loss D: -8.7047, loss G: 119.2951\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 130/252                   Loss D: -8.5321, loss G: 118.8683\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 140/252                   Loss D: -10.6783, loss G: 117.1877\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 150/252                   Loss D: -9.4756, loss G: 116.5599\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 160/252                   Loss D: -6.7351, loss G: 122.1662\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 170/252                   Loss D: -8.5895, loss G: 116.4095\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 180/252                   Loss D: -6.4336, loss G: 114.8197\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 190/252                   Loss D: -7.4442, loss G: 114.9929\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 200/252                   Loss D: -6.4980, loss G: 117.1748\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 210/252                   Loss D: -6.9626, loss G: 113.3384\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 220/252                   Loss D: -11.2388, loss G: 114.0633\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 230/252                   Loss D: -6.9889, loss G: 112.0025\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 240/252                   Loss D: -8.9310, loss G: 112.4655\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/100] Batch 250/252                   Loss D: -6.5758, loss G: 109.0591\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 10/252                   Loss D: -9.4679, loss G: 110.1304\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 20/252                   Loss D: -6.7494, loss G: 110.8330\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 30/252                   Loss D: -8.4388, loss G: 113.0506\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 40/252                   Loss D: -6.2693, loss G: 110.7928\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 50/252                   Loss D: -7.3737, loss G: 112.3931\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 60/252                   Loss D: -4.8908, loss G: 109.8342\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 70/252                   Loss D: -7.9535, loss G: 114.1329\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 80/252                   Loss D: -6.5700, loss G: 113.5958\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 90/252                   Loss D: -6.7782, loss G: 107.6939\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 100/252                   Loss D: -8.8583, loss G: 115.0987\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 110/252                   Loss D: -7.7274, loss G: 108.6787\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 120/252                   Loss D: -6.6240, loss G: 109.8451\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 130/252                   Loss D: -6.1538, loss G: 110.4659\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 140/252                   Loss D: -6.2697, loss G: 104.5545\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 150/252                   Loss D: -5.2239, loss G: 108.0535\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 160/252                   Loss D: -7.1523, loss G: 107.2025\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 170/252                   Loss D: -8.4813, loss G: 106.5804\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 180/252                   Loss D: -6.5631, loss G: 108.6366\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 190/252                   Loss D: -5.8711, loss G: 107.5104\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 200/252                   Loss D: -8.7902, loss G: 97.4431\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 210/252                   Loss D: -8.8725, loss G: 103.2554\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 220/252                   Loss D: -7.3598, loss G: 104.4476\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 230/252                   Loss D: -5.7308, loss G: 97.7194\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 240/252                   Loss D: -6.0713, loss G: 107.0458\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/100] Batch 250/252                   Loss D: -10.5930, loss G: 106.0177\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 10/252                   Loss D: -6.1948, loss G: 106.4979\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 20/252                   Loss D: -7.2689, loss G: 108.5860\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 30/252                   Loss D: -6.4456, loss G: 101.5198\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 40/252                   Loss D: -8.3535, loss G: 97.0765\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 50/252                   Loss D: -7.0257, loss G: 109.5456\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 60/252                   Loss D: -6.1824, loss G: 108.5205\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 70/252                   Loss D: -7.6337, loss G: 104.5481\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 80/252                   Loss D: -6.7327, loss G: 99.9729\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 90/252                   Loss D: -6.1890, loss G: 111.0493\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 100/252                   Loss D: -6.1127, loss G: 108.7428\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 110/252                   Loss D: -6.6463, loss G: 97.0768\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 120/252                   Loss D: -6.5514, loss G: 106.4169\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 130/252                   Loss D: -5.2931, loss G: 98.9726\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 140/252                   Loss D: -6.7298, loss G: 102.4730\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 150/252                   Loss D: -7.7522, loss G: 98.2478\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 160/252                   Loss D: -6.6107, loss G: 102.9645\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 170/252                   Loss D: -5.5079, loss G: 104.7208\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 180/252                   Loss D: -7.4671, loss G: 108.2724\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 190/252                   Loss D: -7.0484, loss G: 99.0384\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 200/252                   Loss D: -6.6412, loss G: 106.3217\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 210/252                   Loss D: -5.2828, loss G: 107.6261\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 220/252                   Loss D: -6.1489, loss G: 105.3505\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 230/252                   Loss D: -5.5586, loss G: 103.1489\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 240/252                   Loss D: -6.8437, loss G: 99.9045\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/100] Batch 250/252                   Loss D: -6.2960, loss G: 98.0758\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 10/252                   Loss D: -5.3721, loss G: 106.6338\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 20/252                   Loss D: -5.2528, loss G: 105.2008\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 30/252                   Loss D: -6.1786, loss G: 95.7605\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 40/252                   Loss D: -7.3760, loss G: 103.5182\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 50/252                   Loss D: -6.7456, loss G: 102.7703\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 60/252                   Loss D: -8.4977, loss G: 100.9203\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 70/252                   Loss D: -6.6438, loss G: 102.5112\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 80/252                   Loss D: -6.8524, loss G: 103.4525\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 90/252                   Loss D: -6.9954, loss G: 102.7439\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 100/252                   Loss D: -8.1601, loss G: 92.9265\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 110/252                   Loss D: -6.8415, loss G: 105.7862\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 120/252                   Loss D: -6.7986, loss G: 99.7892\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 130/252                   Loss D: -5.1098, loss G: 102.2649\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 140/252                   Loss D: -5.4687, loss G: 100.1809\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 150/252                   Loss D: -5.9998, loss G: 104.0421\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 160/252                   Loss D: -6.4857, loss G: 102.6223\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 170/252                   Loss D: -7.4638, loss G: 104.5486\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 180/252                   Loss D: -8.3586, loss G: 107.3510\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 190/252                   Loss D: -5.7397, loss G: 101.6528\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 200/252                   Loss D: -4.8473, loss G: 107.6836\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 210/252                   Loss D: -6.1043, loss G: 101.4568\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 220/252                   Loss D: -6.4993, loss G: 99.4205\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 230/252                   Loss D: -4.9300, loss G: 100.4767\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 240/252                   Loss D: -6.5249, loss G: 100.8744\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/100] Batch 250/252                   Loss D: -6.9606, loss G: 95.8647\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 10/252                   Loss D: -6.1622, loss G: 99.4583\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 20/252                   Loss D: -7.0394, loss G: 92.3960\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 30/252                   Loss D: -5.8430, loss G: 97.7459\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 40/252                   Loss D: -7.5080, loss G: 101.4953\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 50/252                   Loss D: -8.1085, loss G: 98.6100\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 60/252                   Loss D: -4.8635, loss G: 95.7213\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 70/252                   Loss D: -4.6148, loss G: 101.1284\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 80/252                   Loss D: -6.5827, loss G: 92.7127\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 90/252                   Loss D: -5.5717, loss G: 106.3816\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 100/252                   Loss D: -6.3686, loss G: 94.6808\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 110/252                   Loss D: -6.7355, loss G: 100.6527\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 120/252                   Loss D: -6.1559, loss G: 105.3257\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 130/252                   Loss D: -7.5103, loss G: 106.8365\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 140/252                   Loss D: -6.1557, loss G: 99.3101\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 150/252                   Loss D: -6.7668, loss G: 106.2644\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 160/252                   Loss D: -5.4642, loss G: 110.5558\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 170/252                   Loss D: -7.4794, loss G: 106.5999\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 180/252                   Loss D: -5.5171, loss G: 103.4185\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 190/252                   Loss D: -5.3345, loss G: 104.5875\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 200/252                   Loss D: -5.5873, loss G: 95.8611\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 210/252                   Loss D: -5.5101, loss G: 95.3738\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 220/252                   Loss D: -6.0300, loss G: 104.6129\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 230/252                   Loss D: -4.8075, loss G: 99.7669\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 240/252                   Loss D: -6.5583, loss G: 102.2442\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/100] Batch 250/252                   Loss D: -5.8572, loss G: 103.2067\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 10/252                   Loss D: -8.1800, loss G: 98.2182\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 20/252                   Loss D: -5.7200, loss G: 101.6769\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 30/252                   Loss D: -5.7495, loss G: 97.5015\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 40/252                   Loss D: -4.7041, loss G: 96.7211\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 50/252                   Loss D: -5.1332, loss G: 105.1086\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 60/252                   Loss D: -7.4650, loss G: 93.6254\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 70/252                   Loss D: -6.2188, loss G: 103.7009\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 80/252                   Loss D: -5.2104, loss G: 104.1585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 90/252                   Loss D: -6.6003, loss G: 99.1735\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 100/252                   Loss D: -5.7525, loss G: 102.2820\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 110/252                   Loss D: -4.9526, loss G: 105.1219\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 120/252                   Loss D: -6.0408, loss G: 99.2999\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 130/252                   Loss D: -6.0501, loss G: 98.8221\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 140/252                   Loss D: -7.2879, loss G: 95.3859\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 150/252                   Loss D: -8.1951, loss G: 91.7249\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 160/252                   Loss D: -5.2313, loss G: 99.4766\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 170/252                   Loss D: -8.9779, loss G: 101.7988\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 180/252                   Loss D: -5.8741, loss G: 101.5801\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 190/252                   Loss D: -7.0453, loss G: 93.5889\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 200/252                   Loss D: -5.7721, loss G: 99.9253\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 210/252                   Loss D: -6.2769, loss G: 106.1545\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 220/252                   Loss D: -5.1693, loss G: 99.2190\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 230/252                   Loss D: -5.8143, loss G: 92.3066\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 240/252                   Loss D: -6.2366, loss G: 99.1423\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/100] Batch 250/252                   Loss D: -5.7403, loss G: 105.9477\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 10/252                   Loss D: -7.0695, loss G: 105.6056\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 20/252                   Loss D: -5.4643, loss G: 100.3849\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 30/252                   Loss D: -5.2969, loss G: 101.3666\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 40/252                   Loss D: -6.4618, loss G: 92.2688\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 50/252                   Loss D: -9.3457, loss G: 102.9526\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 60/252                   Loss D: -6.4387, loss G: 96.6140\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 70/252                   Loss D: -7.7179, loss G: 109.4331\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 80/252                   Loss D: -4.4149, loss G: 105.7090\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 90/252                   Loss D: -9.9871, loss G: 103.4462\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 100/252                   Loss D: -4.9416, loss G: 93.9709\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 110/252                   Loss D: -5.8058, loss G: 98.6892\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 120/252                   Loss D: -6.0997, loss G: 98.7585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 130/252                   Loss D: -7.3656, loss G: 99.7434\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 140/252                   Loss D: -6.4871, loss G: 99.5342\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 150/252                   Loss D: -7.0074, loss G: 104.3054\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 160/252                   Loss D: -6.4578, loss G: 98.8122\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 170/252                   Loss D: -6.6504, loss G: 98.3771\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 180/252                   Loss D: -6.8393, loss G: 97.7766\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 190/252                   Loss D: -5.1341, loss G: 100.9047\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 200/252                   Loss D: -6.6233, loss G: 100.5535\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 210/252                   Loss D: -5.6681, loss G: 102.0956\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 220/252                   Loss D: -9.9225, loss G: 95.7950\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 230/252                   Loss D: -7.6368, loss G: 98.8645\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 240/252                   Loss D: -6.2554, loss G: 96.6952\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/100] Batch 250/252                   Loss D: -7.5247, loss G: 103.0007\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 10/252                   Loss D: -6.3665, loss G: 100.5449\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 20/252                   Loss D: -5.1548, loss G: 91.3582\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 30/252                   Loss D: -5.5692, loss G: 95.2511\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 40/252                   Loss D: -6.9157, loss G: 101.5352\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 50/252                   Loss D: -5.4483, loss G: 98.0544\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 60/252                   Loss D: -5.4552, loss G: 96.6413\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 70/252                   Loss D: -6.0623, loss G: 105.3983\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 80/252                   Loss D: -5.8833, loss G: 94.5018\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 90/252                   Loss D: -4.3123, loss G: 89.8884\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 100/252                   Loss D: -8.9516, loss G: 84.2729\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 110/252                   Loss D: -5.0555, loss G: 96.0071\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 120/252                   Loss D: -8.1396, loss G: 102.9614\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 130/252                   Loss D: -6.0369, loss G: 99.8582\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 140/252                   Loss D: -8.7344, loss G: 93.5771\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 150/252                   Loss D: -9.0085, loss G: 100.4860\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 160/252                   Loss D: -5.3029, loss G: 102.3174\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 170/252                   Loss D: -5.1645, loss G: 94.9813\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 180/252                   Loss D: -7.7897, loss G: 98.4992\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 190/252                   Loss D: -5.7044, loss G: 94.7264\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 200/252                   Loss D: -6.8941, loss G: 100.6034\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 210/252                   Loss D: -7.0841, loss G: 101.8533\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 220/252                   Loss D: -4.8267, loss G: 95.3772\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 230/252                   Loss D: -5.6312, loss G: 93.4939\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 240/252                   Loss D: -7.9902, loss G: 102.3041\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [11/100] Batch 250/252                   Loss D: -8.1619, loss G: 89.5541\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 10/252                   Loss D: -6.9892, loss G: 97.1381\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 20/252                   Loss D: -6.1101, loss G: 101.2894\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 30/252                   Loss D: -6.3082, loss G: 96.7023\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 40/252                   Loss D: -5.3919, loss G: 97.3795\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 50/252                   Loss D: -7.3205, loss G: 85.6210\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 60/252                   Loss D: -6.5584, loss G: 99.9537\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 70/252                   Loss D: -7.0198, loss G: 95.7989\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 80/252                   Loss D: -4.8364, loss G: 103.5824\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 90/252                   Loss D: -5.1273, loss G: 97.9159\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 100/252                   Loss D: -6.4479, loss G: 99.1491\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 110/252                   Loss D: -6.8837, loss G: 103.3154\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 120/252                   Loss D: -6.2586, loss G: 90.2412\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 130/252                   Loss D: -5.2467, loss G: 96.7878\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 140/252                   Loss D: -8.1720, loss G: 90.4765\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 150/252                   Loss D: -5.1229, loss G: 88.5632\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 160/252                   Loss D: -5.8707, loss G: 93.8589\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 170/252                   Loss D: -9.0314, loss G: 93.3775\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 180/252                   Loss D: -5.1395, loss G: 96.4068\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 190/252                   Loss D: -4.7315, loss G: 102.0704\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 200/252                   Loss D: -4.7793, loss G: 101.1190\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 210/252                   Loss D: -6.6116, loss G: 100.9306\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 220/252                   Loss D: -7.0421, loss G: 99.9753\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 230/252                   Loss D: -4.3528, loss G: 103.0348\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 240/252                   Loss D: -8.3319, loss G: 102.5714\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [12/100] Batch 250/252                   Loss D: -5.4445, loss G: 94.4161\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 10/252                   Loss D: -5.8969, loss G: 87.4565\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 20/252                   Loss D: -6.7054, loss G: 100.0460\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 30/252                   Loss D: -9.0106, loss G: 103.8979\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 40/252                   Loss D: -6.1475, loss G: 98.2303\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 50/252                   Loss D: -8.6344, loss G: 85.9648\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 60/252                   Loss D: -6.9118, loss G: 96.6852\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 70/252                   Loss D: -7.4454, loss G: 102.6413\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 80/252                   Loss D: -4.9499, loss G: 97.8311\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 90/252                   Loss D: -6.9285, loss G: 100.2912\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 100/252                   Loss D: -5.6801, loss G: 96.9001\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 110/252                   Loss D: -6.6249, loss G: 93.9817\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 120/252                   Loss D: -5.6656, loss G: 93.3066\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 130/252                   Loss D: -7.7741, loss G: 98.3074\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 140/252                   Loss D: -5.3972, loss G: 94.9355\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 150/252                   Loss D: -4.3792, loss G: 92.4655\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 160/252                   Loss D: -6.8746, loss G: 103.3715\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 170/252                   Loss D: -6.3336, loss G: 94.4332\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 180/252                   Loss D: -6.5474, loss G: 100.2426\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 190/252                   Loss D: -6.6113, loss G: 94.2327\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 200/252                   Loss D: -4.9961, loss G: 97.7553\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 210/252                   Loss D: -5.0898, loss G: 98.3450\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 220/252                   Loss D: -6.0057, loss G: 98.5677\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 230/252                   Loss D: -6.6673, loss G: 91.4162\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 240/252                   Loss D: -4.2991, loss G: 98.9247\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [13/100] Batch 250/252                   Loss D: -5.8845, loss G: 101.2143\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 10/252                   Loss D: -5.9918, loss G: 98.5545\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 20/252                   Loss D: -6.5399, loss G: 106.2803\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 30/252                   Loss D: -5.7037, loss G: 91.6182\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 40/252                   Loss D: -6.3001, loss G: 93.5897\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 50/252                   Loss D: -5.6335, loss G: 98.0767\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 60/252                   Loss D: -7.6936, loss G: 100.5890\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 70/252                   Loss D: -4.8652, loss G: 95.3463\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 80/252                   Loss D: -4.3482, loss G: 98.2890\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 90/252                   Loss D: -5.4365, loss G: 98.3211\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 100/252                   Loss D: -4.2390, loss G: 97.6641\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 110/252                   Loss D: -5.6226, loss G: 91.2534\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 120/252                   Loss D: -6.9504, loss G: 94.8191\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 130/252                   Loss D: -3.9300, loss G: 94.4325\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 140/252                   Loss D: -4.3845, loss G: 101.3686\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 150/252                   Loss D: -4.2700, loss G: 90.9830\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 160/252                   Loss D: -8.4884, loss G: 102.4867\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 170/252                   Loss D: -5.9648, loss G: 94.1837\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 180/252                   Loss D: -4.1109, loss G: 95.1661\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 190/252                   Loss D: -5.2741, loss G: 98.4834\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 200/252                   Loss D: -6.6827, loss G: 93.9987\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 210/252                   Loss D: -4.6205, loss G: 96.2241\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 220/252                   Loss D: -5.0362, loss G: 93.7728\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 230/252                   Loss D: -2.3897, loss G: 89.3037\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 240/252                   Loss D: -4.2516, loss G: 101.1685\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [14/100] Batch 250/252                   Loss D: -4.3991, loss G: 96.6177\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 10/252                   Loss D: -5.9794, loss G: 98.3887\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 20/252                   Loss D: -8.0903, loss G: 88.7598\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 30/252                   Loss D: -4.9143, loss G: 94.1851\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 40/252                   Loss D: -5.6361, loss G: 93.3803\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 50/252                   Loss D: -5.2122, loss G: 93.4097\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 60/252                   Loss D: -5.9865, loss G: 96.8815\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 70/252                   Loss D: -7.1102, loss G: 90.5754\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 80/252                   Loss D: -4.7173, loss G: 94.3193\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 90/252                   Loss D: -4.3642, loss G: 99.1841\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 100/252                   Loss D: -3.9177, loss G: 98.0418\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 110/252                   Loss D: -4.6325, loss G: 96.5412\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 120/252                   Loss D: -3.2841, loss G: 97.9732\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 130/252                   Loss D: -4.6317, loss G: 90.7271\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 140/252                   Loss D: -4.1294, loss G: 100.0990\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 150/252                   Loss D: -5.8461, loss G: 98.0283\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 160/252                   Loss D: -4.8153, loss G: 96.8119\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 170/252                   Loss D: -6.7809, loss G: 91.8142\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 180/252                   Loss D: -5.5490, loss G: 92.4147\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 190/252                   Loss D: -6.5582, loss G: 94.8267\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 200/252                   Loss D: -5.1634, loss G: 99.4211\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 210/252                   Loss D: -4.5131, loss G: 96.3149\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 220/252                   Loss D: -6.2049, loss G: 96.3168\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 230/252                   Loss D: -4.8323, loss G: 94.4714\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 240/252                   Loss D: -4.6287, loss G: 93.2647\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [15/100] Batch 250/252                   Loss D: -5.7264, loss G: 86.3210\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 10/252                   Loss D: -5.2990, loss G: 96.9661\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 20/252                   Loss D: -6.9285, loss G: 99.5404\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 30/252                   Loss D: -4.0547, loss G: 93.0950\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 40/252                   Loss D: -5.0422, loss G: 91.1982\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 50/252                   Loss D: -6.6586, loss G: 94.0791\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 60/252                   Loss D: -8.4286, loss G: 102.0185\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 70/252                   Loss D: -5.3102, loss G: 97.3145\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 80/252                   Loss D: -5.0124, loss G: 94.3122\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 90/252                   Loss D: -4.1856, loss G: 98.4447\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 100/252                   Loss D: -4.3197, loss G: 94.4915\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 110/252                   Loss D: -6.8246, loss G: 95.6396\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 120/252                   Loss D: -6.0783, loss G: 102.7453\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 130/252                   Loss D: -4.4573, loss G: 96.7286\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 140/252                   Loss D: -5.8393, loss G: 96.6412\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 150/252                   Loss D: -5.8141, loss G: 92.5443\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 160/252                   Loss D: -4.3440, loss G: 87.8547\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 170/252                   Loss D: -6.1798, loss G: 97.5177\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 180/252                   Loss D: -7.7443, loss G: 100.4723\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 190/252                   Loss D: -4.6965, loss G: 89.8254\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 200/252                   Loss D: -6.9270, loss G: 96.9120\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 210/252                   Loss D: -3.8710, loss G: 100.7836\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 220/252                   Loss D: -9.6383, loss G: 95.2816\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 230/252                   Loss D: -4.8169, loss G: 91.0207\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 240/252                   Loss D: -7.0811, loss G: 92.8099\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [16/100] Batch 250/252                   Loss D: -7.3041, loss G: 93.4784\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 10/252                   Loss D: -6.9261, loss G: 98.4148\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 20/252                   Loss D: -3.7660, loss G: 94.6528\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 30/252                   Loss D: -6.5907, loss G: 91.4017\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 40/252                   Loss D: -5.0015, loss G: 91.7165\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 50/252                   Loss D: -5.4370, loss G: 93.6514\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 60/252                   Loss D: -7.3404, loss G: 94.5284\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 70/252                   Loss D: -6.4645, loss G: 90.9218\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 80/252                   Loss D: -2.8229, loss G: 97.2252\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 90/252                   Loss D: -4.9903, loss G: 98.6172\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 100/252                   Loss D: -4.0346, loss G: 96.8935\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 110/252                   Loss D: -4.3935, loss G: 95.6479\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 120/252                   Loss D: -5.4786, loss G: 96.3856\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 130/252                   Loss D: -4.3748, loss G: 93.4370\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 140/252                   Loss D: -5.8086, loss G: 97.0909\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 150/252                   Loss D: -5.6493, loss G: 94.9498\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 160/252                   Loss D: -8.6595, loss G: 94.7341\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 170/252                   Loss D: -7.3781, loss G: 94.8677\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 180/252                   Loss D: -3.6597, loss G: 97.8716\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 190/252                   Loss D: -4.8483, loss G: 89.1304\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 200/252                   Loss D: -4.9576, loss G: 96.0405\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 210/252                   Loss D: -4.5105, loss G: 94.2046\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 220/252                   Loss D: -7.4573, loss G: 92.7646\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 230/252                   Loss D: -6.8934, loss G: 94.7759\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 240/252                   Loss D: -5.0635, loss G: 96.6492\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [17/100] Batch 250/252                   Loss D: -4.1044, loss G: 97.8123\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 10/252                   Loss D: -6.1819, loss G: 92.3143\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 20/252                   Loss D: -4.0204, loss G: 100.3969\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 30/252                   Loss D: -4.0067, loss G: 98.5066\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 40/252                   Loss D: -8.4401, loss G: 95.2948\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 50/252                   Loss D: -5.5427, loss G: 96.5515\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 60/252                   Loss D: -6.9532, loss G: 92.0283\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 70/252                   Loss D: -5.2689, loss G: 97.3654\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 80/252                   Loss D: -5.8730, loss G: 90.8608\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 90/252                   Loss D: -6.3178, loss G: 98.7229\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 100/252                   Loss D: -7.1299, loss G: 96.5583\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 110/252                   Loss D: -3.7400, loss G: 91.1821\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 120/252                   Loss D: -5.8772, loss G: 93.7853\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 130/252                   Loss D: -5.3501, loss G: 92.8886\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 140/252                   Loss D: -6.6648, loss G: 96.5715\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 150/252                   Loss D: -6.6926, loss G: 93.5511\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 160/252                   Loss D: -4.1111, loss G: 94.1945\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 170/252                   Loss D: -5.0770, loss G: 93.5599\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 180/252                   Loss D: -4.8482, loss G: 97.7294\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 190/252                   Loss D: -5.9538, loss G: 100.9149\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 200/252                   Loss D: -4.5341, loss G: 92.9452\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 210/252                   Loss D: -4.7452, loss G: 93.0620\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 220/252                   Loss D: -6.9560, loss G: 91.7923\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 230/252                   Loss D: -4.0217, loss G: 92.5624\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 240/252                   Loss D: -2.3971, loss G: 93.9985\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [18/100] Batch 250/252                   Loss D: -5.7832, loss G: 91.1349\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 10/252                   Loss D: -5.4740, loss G: 97.9271\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 20/252                   Loss D: -5.0773, loss G: 99.0078\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 30/252                   Loss D: -5.4301, loss G: 95.1772\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 40/252                   Loss D: -4.2071, loss G: 94.9230\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 50/252                   Loss D: -9.2448, loss G: 96.6345\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 60/252                   Loss D: -5.4959, loss G: 90.5924\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 70/252                   Loss D: -5.0137, loss G: 97.8946\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 80/252                   Loss D: -6.1470, loss G: 100.0292\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 90/252                   Loss D: -6.0871, loss G: 90.7458\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 100/252                   Loss D: -6.8829, loss G: 94.4630\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 110/252                   Loss D: -4.5733, loss G: 96.9138\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 120/252                   Loss D: -3.4607, loss G: 93.7804\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 130/252                   Loss D: -7.7886, loss G: 90.6255\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 140/252                   Loss D: -5.0047, loss G: 96.4991\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 150/252                   Loss D: -4.3360, loss G: 93.4824\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 160/252                   Loss D: -4.2764, loss G: 90.9107\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 170/252                   Loss D: -4.2527, loss G: 93.6742\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 180/252                   Loss D: -5.0834, loss G: 97.5536\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 190/252                   Loss D: -5.4134, loss G: 97.5000\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 200/252                   Loss D: -5.5100, loss G: 97.3561\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 210/252                   Loss D: -4.4096, loss G: 94.2476\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 220/252                   Loss D: -5.5253, loss G: 97.5154\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 230/252                   Loss D: -6.4208, loss G: 100.6241\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 240/252                   Loss D: -3.8774, loss G: 92.2315\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [19/100] Batch 250/252                   Loss D: -5.0798, loss G: 97.7172\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 10/252                   Loss D: -7.6759, loss G: 95.9209\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 20/252                   Loss D: -5.6731, loss G: 94.7300\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 30/252                   Loss D: -5.8787, loss G: 90.8695\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 40/252                   Loss D: -5.7202, loss G: 97.4753\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 50/252                   Loss D: -4.4927, loss G: 95.2904\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 60/252                   Loss D: -5.7182, loss G: 88.6040\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 70/252                   Loss D: -6.0900, loss G: 96.4182\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 80/252                   Loss D: -4.4109, loss G: 92.1243\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 90/252                   Loss D: -4.9538, loss G: 98.2466\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 100/252                   Loss D: -4.3194, loss G: 92.2371\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 110/252                   Loss D: -4.0793, loss G: 97.5837\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 120/252                   Loss D: -4.0551, loss G: 96.0283\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 130/252                   Loss D: -5.8051, loss G: 97.0117\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 140/252                   Loss D: -4.8581, loss G: 94.6466\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 150/252                   Loss D: -5.4147, loss G: 94.7704\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 160/252                   Loss D: -7.8966, loss G: 89.3073\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 170/252                   Loss D: -5.7307, loss G: 96.1136\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 180/252                   Loss D: -6.0166, loss G: 93.7187\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 190/252                   Loss D: -4.4311, loss G: 89.6449\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 200/252                   Loss D: -7.7752, loss G: 99.8779\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 210/252                   Loss D: -4.5451, loss G: 92.6928\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 220/252                   Loss D: -4.2199, loss G: 99.2020\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 230/252                   Loss D: -5.0889, loss G: 93.8232\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 240/252                   Loss D: -5.8471, loss G: 92.9207\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [20/100] Batch 250/252                   Loss D: -6.4854, loss G: 94.4758\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 10/252                   Loss D: -5.4790, loss G: 92.6975\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 20/252                   Loss D: -4.4348, loss G: 92.4008\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 30/252                   Loss D: -3.8528, loss G: 96.6333\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 40/252                   Loss D: -3.5516, loss G: 98.3958\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 50/252                   Loss D: -4.5072, loss G: 100.7592\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 60/252                   Loss D: -5.5127, loss G: 95.5869\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 70/252                   Loss D: -8.2152, loss G: 90.5965\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 80/252                   Loss D: -3.6314, loss G: 95.6792\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 90/252                   Loss D: -3.9830, loss G: 93.8204\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 100/252                   Loss D: -8.3738, loss G: 91.2313\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 110/252                   Loss D: -4.9964, loss G: 96.8764\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 120/252                   Loss D: -6.0581, loss G: 103.4694\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 130/252                   Loss D: -3.5842, loss G: 95.2066\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 140/252                   Loss D: -5.6629, loss G: 98.9268\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 150/252                   Loss D: -6.2517, loss G: 94.8973\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 160/252                   Loss D: -5.0486, loss G: 98.8202\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 170/252                   Loss D: -5.6244, loss G: 96.6995\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 180/252                   Loss D: -5.9915, loss G: 106.3173\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 190/252                   Loss D: -6.9264, loss G: 93.9251\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 200/252                   Loss D: -5.2816, loss G: 95.6835\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 210/252                   Loss D: -4.9954, loss G: 97.3857\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 220/252                   Loss D: -11.2010, loss G: 97.0392\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 230/252                   Loss D: -5.9734, loss G: 96.1580\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 240/252                   Loss D: -5.2643, loss G: 101.8253\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [21/100] Batch 250/252                   Loss D: -5.2126, loss G: 106.0022\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 10/252                   Loss D: -5.9899, loss G: 98.4926\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 20/252                   Loss D: -5.0838, loss G: 95.6142\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 30/252                   Loss D: -5.4555, loss G: 99.3647\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 40/252                   Loss D: -8.3091, loss G: 98.0871\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 50/252                   Loss D: -4.8019, loss G: 97.0567\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 60/252                   Loss D: -3.5247, loss G: 98.2885\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 70/252                   Loss D: -6.6938, loss G: 99.5184\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 80/252                   Loss D: -7.8936, loss G: 96.9121\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 90/252                   Loss D: -4.6423, loss G: 97.8502\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 100/252                   Loss D: -6.0195, loss G: 88.6288\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 110/252                   Loss D: -7.1327, loss G: 94.0703\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 120/252                   Loss D: -4.9161, loss G: 94.8234\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 130/252                   Loss D: -4.2049, loss G: 100.5010\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 140/252                   Loss D: -6.0957, loss G: 93.0413\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 150/252                   Loss D: -5.1926, loss G: 98.4007\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 160/252                   Loss D: -7.0619, loss G: 102.5719\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 170/252                   Loss D: -5.7001, loss G: 94.5735\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 180/252                   Loss D: -7.4557, loss G: 100.4866\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 190/252                   Loss D: -5.9974, loss G: 94.1003\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 200/252                   Loss D: -4.2011, loss G: 93.5687\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 210/252                   Loss D: -3.6595, loss G: 99.5595\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 220/252                   Loss D: -5.9255, loss G: 94.4458\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 230/252                   Loss D: -5.0640, loss G: 99.9178\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 240/252                   Loss D: -6.7853, loss G: 96.1010\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [22/100] Batch 250/252                   Loss D: -7.8176, loss G: 91.6317\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 10/252                   Loss D: -4.7133, loss G: 99.1667\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 20/252                   Loss D: -5.3428, loss G: 93.0620\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 30/252                   Loss D: -5.7758, loss G: 94.8596\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 40/252                   Loss D: -4.6059, loss G: 93.7224\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 50/252                   Loss D: -6.3715, loss G: 97.9469\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 60/252                   Loss D: -5.4119, loss G: 97.1884\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 70/252                   Loss D: -6.7282, loss G: 95.4894\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 80/252                   Loss D: -6.0550, loss G: 96.1904\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 90/252                   Loss D: -7.4847, loss G: 100.6863\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 100/252                   Loss D: -6.2699, loss G: 95.6548\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 110/252                   Loss D: -6.2427, loss G: 105.1508\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 120/252                   Loss D: -7.6019, loss G: 101.6315\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 130/252                   Loss D: -6.9351, loss G: 100.0681\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 140/252                   Loss D: -6.5445, loss G: 100.4886\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 150/252                   Loss D: -6.8053, loss G: 100.7434\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 160/252                   Loss D: -6.5508, loss G: 103.1461\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 170/252                   Loss D: -5.5084, loss G: 98.5448\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 180/252                   Loss D: -5.1632, loss G: 97.6368\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 190/252                   Loss D: -3.8459, loss G: 98.5536\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 200/252                   Loss D: -5.0150, loss G: 101.1628\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 210/252                   Loss D: -6.1093, loss G: 100.8020\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 220/252                   Loss D: -4.8740, loss G: 100.1192\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 230/252                   Loss D: -3.8014, loss G: 91.5063\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 240/252                   Loss D: -5.7969, loss G: 96.7487\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [23/100] Batch 250/252                   Loss D: -3.8402, loss G: 97.6565\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 10/252                   Loss D: -5.0250, loss G: 94.6811\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 20/252                   Loss D: -5.0958, loss G: 105.2170\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 30/252                   Loss D: -5.8406, loss G: 94.2507\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 40/252                   Loss D: -6.2598, loss G: 99.1543\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 50/252                   Loss D: -5.0589, loss G: 98.7444\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 60/252                   Loss D: -7.1162, loss G: 99.3922\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 70/252                   Loss D: -4.1632, loss G: 94.6175\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 80/252                   Loss D: -4.6407, loss G: 99.3325\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 90/252                   Loss D: -6.8501, loss G: 104.9941\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 100/252                   Loss D: -4.3903, loss G: 95.3954\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 110/252                   Loss D: -5.5561, loss G: 93.5838\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 120/252                   Loss D: -5.5156, loss G: 95.8051\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 130/252                   Loss D: -4.6943, loss G: 99.7976\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 140/252                   Loss D: -5.9920, loss G: 99.9284\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 150/252                   Loss D: -6.3149, loss G: 104.2858\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 160/252                   Loss D: -6.8725, loss G: 97.5033\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 170/252                   Loss D: -4.3798, loss G: 94.1481\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 180/252                   Loss D: -7.6940, loss G: 99.5298\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 190/252                   Loss D: -7.1499, loss G: 92.7611\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 200/252                   Loss D: -5.2687, loss G: 98.4327\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 210/252                   Loss D: -6.5971, loss G: 97.6284\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 220/252                   Loss D: -6.0282, loss G: 97.3537\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 230/252                   Loss D: -6.7553, loss G: 99.1245\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 240/252                   Loss D: -4.0712, loss G: 99.2401\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [24/100] Batch 250/252                   Loss D: -4.3169, loss G: 101.6496\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 10/252                   Loss D: -6.4587, loss G: 94.9296\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 20/252                   Loss D: -4.4487, loss G: 102.3647\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 30/252                   Loss D: -7.3961, loss G: 102.8727\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 40/252                   Loss D: -7.2953, loss G: 102.9395\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 50/252                   Loss D: -5.5902, loss G: 99.4448\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 60/252                   Loss D: -5.9848, loss G: 97.9085\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 70/252                   Loss D: -7.3849, loss G: 96.9577\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 80/252                   Loss D: -4.4187, loss G: 102.1145\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 90/252                   Loss D: -6.4839, loss G: 102.8094\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 100/252                   Loss D: -7.4482, loss G: 101.2165\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 110/252                   Loss D: -4.3920, loss G: 96.9613\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 120/252                   Loss D: -6.8602, loss G: 95.1284\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 130/252                   Loss D: -5.7668, loss G: 100.0779\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 140/252                   Loss D: -5.3867, loss G: 96.0835\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 150/252                   Loss D: -5.8280, loss G: 95.0585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 160/252                   Loss D: -5.2839, loss G: 102.6518\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 170/252                   Loss D: -5.9301, loss G: 101.9030\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 180/252                   Loss D: -3.9197, loss G: 101.9602\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 190/252                   Loss D: -8.6614, loss G: 93.8970\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 200/252                   Loss D: -4.3477, loss G: 96.7664\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 210/252                   Loss D: -6.5069, loss G: 107.8703\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 220/252                   Loss D: -5.1618, loss G: 97.3829\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 230/252                   Loss D: -4.7108, loss G: 99.4473\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 240/252                   Loss D: -6.3586, loss G: 96.3171\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [25/100] Batch 250/252                   Loss D: -6.3817, loss G: 102.8779\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 10/252                   Loss D: -8.6958, loss G: 99.0907\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 20/252                   Loss D: -4.8965, loss G: 104.0198\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 30/252                   Loss D: -4.8661, loss G: 97.4302\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 40/252                   Loss D: -6.4646, loss G: 96.3260\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 50/252                   Loss D: -4.5989, loss G: 97.9387\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 60/252                   Loss D: -6.2896, loss G: 102.1583\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 70/252                   Loss D: -5.5687, loss G: 99.2356\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 80/252                   Loss D: -5.8543, loss G: 99.3061\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 90/252                   Loss D: -6.7522, loss G: 99.5072\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 100/252                   Loss D: -4.3450, loss G: 103.4599\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 110/252                   Loss D: -4.1579, loss G: 98.6943\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 120/252                   Loss D: -5.5152, loss G: 100.4512\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 130/252                   Loss D: -7.6323, loss G: 96.4625\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 140/252                   Loss D: -4.5642, loss G: 95.3169\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 150/252                   Loss D: -5.6304, loss G: 102.0596\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 160/252                   Loss D: -3.7778, loss G: 95.4448\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 170/252                   Loss D: -5.5904, loss G: 100.0336\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 180/252                   Loss D: -5.2894, loss G: 101.0843\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 190/252                   Loss D: -4.5920, loss G: 99.2940\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 200/252                   Loss D: -6.0444, loss G: 102.7166\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 210/252                   Loss D: -5.2962, loss G: 102.5792\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 220/252                   Loss D: -5.2999, loss G: 102.0817\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 230/252                   Loss D: -8.1492, loss G: 98.6277\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 240/252                   Loss D: -4.8760, loss G: 102.0585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [26/100] Batch 250/252                   Loss D: -7.2455, loss G: 104.7537\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 10/252                   Loss D: -4.5451, loss G: 97.8449\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 20/252                   Loss D: -6.6383, loss G: 100.3236\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 30/252                   Loss D: -4.5522, loss G: 104.8139\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 40/252                   Loss D: -4.3323, loss G: 98.8434\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 50/252                   Loss D: -8.9312, loss G: 105.9838\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 60/252                   Loss D: -5.3776, loss G: 99.9106\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 70/252                   Loss D: -5.9912, loss G: 98.6712\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 80/252                   Loss D: -5.5257, loss G: 100.0476\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 90/252                   Loss D: -7.8122, loss G: 93.9460\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 100/252                   Loss D: -4.5587, loss G: 98.8623\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 110/252                   Loss D: -5.3071, loss G: 102.6225\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 120/252                   Loss D: -5.7507, loss G: 97.3386\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 130/252                   Loss D: -6.0307, loss G: 98.9898\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 140/252                   Loss D: -4.8375, loss G: 99.3087\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 150/252                   Loss D: -4.5441, loss G: 100.9666\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 160/252                   Loss D: -8.0023, loss G: 102.8514\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 170/252                   Loss D: -5.9228, loss G: 102.9850\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 180/252                   Loss D: -4.1659, loss G: 97.8980\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 190/252                   Loss D: -7.2431, loss G: 107.3998\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 200/252                   Loss D: -5.1987, loss G: 101.9891\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 210/252                   Loss D: -5.5305, loss G: 101.9682\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 220/252                   Loss D: -4.5331, loss G: 99.8074\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 230/252                   Loss D: -5.6100, loss G: 108.3735\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 240/252                   Loss D: -4.7755, loss G: 99.6914\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [27/100] Batch 250/252                   Loss D: -5.3016, loss G: 101.9840\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 10/252                   Loss D: -6.8912, loss G: 98.7179\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 20/252                   Loss D: -5.5413, loss G: 100.0367\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 30/252                   Loss D: -5.6743, loss G: 100.3324\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 40/252                   Loss D: -7.7687, loss G: 105.7966\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 50/252                   Loss D: -5.0620, loss G: 103.3278\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 60/252                   Loss D: -4.4103, loss G: 96.5532\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 70/252                   Loss D: -5.4087, loss G: 106.5735\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 80/252                   Loss D: -4.3602, loss G: 98.4627\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 90/252                   Loss D: -4.3646, loss G: 101.1585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 100/252                   Loss D: -5.3845, loss G: 99.9480\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 110/252                   Loss D: -6.0171, loss G: 106.4158\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 120/252                   Loss D: -5.1712, loss G: 105.3887\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 130/252                   Loss D: -4.7386, loss G: 103.1056\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 140/252                   Loss D: -5.3011, loss G: 104.6395\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 150/252                   Loss D: -5.9846, loss G: 100.3754\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 160/252                   Loss D: -5.2520, loss G: 105.6771\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 170/252                   Loss D: -6.3479, loss G: 104.1983\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 180/252                   Loss D: -5.7670, loss G: 97.6014\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 190/252                   Loss D: -5.9621, loss G: 99.0033\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 200/252                   Loss D: -5.0132, loss G: 101.9257\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 210/252                   Loss D: -5.3687, loss G: 101.1121\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 220/252                   Loss D: -4.8607, loss G: 100.7864\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 230/252                   Loss D: -6.4434, loss G: 105.9273\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 240/252                   Loss D: -4.3108, loss G: 103.0982\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [28/100] Batch 250/252                   Loss D: -8.4809, loss G: 96.1090\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 10/252                   Loss D: -8.3463, loss G: 106.7093\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 20/252                   Loss D: -5.8277, loss G: 100.2554\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 30/252                   Loss D: -7.1878, loss G: 103.1760\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 40/252                   Loss D: -4.1239, loss G: 106.4863\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 50/252                   Loss D: -5.5257, loss G: 101.7066\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 60/252                   Loss D: -4.4849, loss G: 100.7742\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 70/252                   Loss D: -4.4299, loss G: 102.4758\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 80/252                   Loss D: -6.1126, loss G: 106.3021\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 90/252                   Loss D: -4.1211, loss G: 103.8790\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 100/252                   Loss D: -6.0961, loss G: 106.4073\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 110/252                   Loss D: -4.9124, loss G: 99.3677\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 120/252                   Loss D: -6.4022, loss G: 100.2757\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 130/252                   Loss D: -4.9666, loss G: 102.8638\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 140/252                   Loss D: -3.4798, loss G: 101.7142\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 150/252                   Loss D: -4.6840, loss G: 102.8795\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 160/252                   Loss D: -3.4348, loss G: 104.6414\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 170/252                   Loss D: -4.0942, loss G: 104.1173\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 180/252                   Loss D: -4.8614, loss G: 104.3904\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 190/252                   Loss D: -4.4933, loss G: 99.5954\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 200/252                   Loss D: -5.4810, loss G: 110.4120\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 210/252                   Loss D: -5.1730, loss G: 101.4791\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 220/252                   Loss D: -7.3448, loss G: 105.4111\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 230/252                   Loss D: -5.9873, loss G: 103.0853\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 240/252                   Loss D: -5.9914, loss G: 109.2810\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [29/100] Batch 250/252                   Loss D: -6.1511, loss G: 103.4430\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 10/252                   Loss D: -5.7536, loss G: 102.6384\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 20/252                   Loss D: -5.6544, loss G: 107.2758\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 30/252                   Loss D: -6.2631, loss G: 104.1204\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 40/252                   Loss D: -6.9212, loss G: 100.2006\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 50/252                   Loss D: -7.3026, loss G: 108.0613\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 60/252                   Loss D: -6.1322, loss G: 101.9661\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 70/252                   Loss D: -5.5900, loss G: 102.5115\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 80/252                   Loss D: -7.1353, loss G: 101.3682\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 90/252                   Loss D: -5.9913, loss G: 100.7929\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 100/252                   Loss D: -5.6060, loss G: 108.9812\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 110/252                   Loss D: -5.2657, loss G: 100.4163\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 120/252                   Loss D: -6.3315, loss G: 100.9979\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 130/252                   Loss D: -7.6459, loss G: 102.8047\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 140/252                   Loss D: -8.7413, loss G: 109.6701\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 150/252                   Loss D: -4.3138, loss G: 105.1732\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 160/252                   Loss D: -4.8126, loss G: 102.9190\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 170/252                   Loss D: -4.8062, loss G: 103.0759\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 180/252                   Loss D: -4.4898, loss G: 102.2299\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 190/252                   Loss D: -6.6503, loss G: 102.0819\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 200/252                   Loss D: -3.8426, loss G: 103.0371\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 210/252                   Loss D: -5.3305, loss G: 102.7546\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 220/252                   Loss D: -5.2074, loss G: 102.9707\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 230/252                   Loss D: -3.1686, loss G: 102.0125\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 240/252                   Loss D: -6.6794, loss G: 102.5409\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [30/100] Batch 250/252                   Loss D: -4.4038, loss G: 101.5581\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 10/252                   Loss D: -4.9313, loss G: 104.0144\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 20/252                   Loss D: -7.6406, loss G: 105.8745\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 30/252                   Loss D: -6.1002, loss G: 108.4628\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 40/252                   Loss D: -4.8144, loss G: 107.0585\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 50/252                   Loss D: -5.2628, loss G: 104.5744\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 60/252                   Loss D: -4.4317, loss G: 106.0500\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 70/252                   Loss D: -4.8248, loss G: 104.3003\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 80/252                   Loss D: -5.7198, loss G: 107.8609\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 90/252                   Loss D: -5.2474, loss G: 105.3715\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 100/252                   Loss D: -6.0369, loss G: 102.4629\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 110/252                   Loss D: -5.6897, loss G: 104.4420\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 120/252                   Loss D: -5.4880, loss G: 108.3425\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 130/252                   Loss D: -3.5125, loss G: 102.7936\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 140/252                   Loss D: -5.6688, loss G: 105.2558\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 150/252                   Loss D: -4.5111, loss G: 105.9684\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 160/252                   Loss D: -4.2800, loss G: 100.3309\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 170/252                   Loss D: -4.6797, loss G: 102.3858\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 180/252                   Loss D: -6.1629, loss G: 100.0104\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 190/252                   Loss D: -5.0978, loss G: 104.9820\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 200/252                   Loss D: -8.5083, loss G: 101.1349\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 210/252                   Loss D: -5.2487, loss G: 103.9201\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 220/252                   Loss D: -5.5768, loss G: 99.0736\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 230/252                   Loss D: -5.5889, loss G: 106.8203\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 240/252                   Loss D: -5.6977, loss G: 104.7168\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [31/100] Batch 250/252                   Loss D: -4.5168, loss G: 100.5756\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 10/252                   Loss D: -4.6594, loss G: 99.4460\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 20/252                   Loss D: -7.0460, loss G: 104.4118\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 30/252                   Loss D: -7.3082, loss G: 104.1339\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 40/252                   Loss D: -5.6522, loss G: 101.4016\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 50/252                   Loss D: -6.1586, loss G: 109.0087\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 60/252                   Loss D: -5.9823, loss G: 109.7922\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 70/252                   Loss D: -6.8043, loss G: 109.2618\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 80/252                   Loss D: -8.1757, loss G: 108.4731\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 90/252                   Loss D: -4.0910, loss G: 103.5623\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 100/252                   Loss D: -4.3539, loss G: 98.1774\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 110/252                   Loss D: -5.2062, loss G: 104.5917\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 120/252                   Loss D: -5.7321, loss G: 107.6363\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 130/252                   Loss D: -4.6919, loss G: 102.1429\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 140/252                   Loss D: -7.5724, loss G: 103.2030\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 150/252                   Loss D: -5.9645, loss G: 105.3701\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 160/252                   Loss D: -4.9229, loss G: 102.1477\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 170/252                   Loss D: -4.2051, loss G: 100.0175\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 180/252                   Loss D: -3.5023, loss G: 108.1692\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 190/252                   Loss D: -6.0898, loss G: 101.8769\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 200/252                   Loss D: -5.2993, loss G: 105.3481\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 210/252                   Loss D: -4.8097, loss G: 110.0131\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 220/252                   Loss D: -5.9486, loss G: 105.8977\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 230/252                   Loss D: -4.8703, loss G: 105.0061\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 240/252                   Loss D: -5.0459, loss G: 107.9043\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [32/100] Batch 250/252                   Loss D: -4.4905, loss G: 108.7952\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 10/252                   Loss D: -6.5055, loss G: 105.2193\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 20/252                   Loss D: -6.1156, loss G: 108.0593\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 30/252                   Loss D: -6.9143, loss G: 111.2037\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 40/252                   Loss D: -6.7929, loss G: 104.1149\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 50/252                   Loss D: -6.5053, loss G: 104.4890\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 60/252                   Loss D: -5.4774, loss G: 104.3584\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 70/252                   Loss D: -5.9491, loss G: 107.3397\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 80/252                   Loss D: -3.4307, loss G: 103.2193\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 90/252                   Loss D: -6.3127, loss G: 108.2233\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 100/252                   Loss D: -5.5633, loss G: 104.7678\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 110/252                   Loss D: -6.9288, loss G: 108.1285\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 120/252                   Loss D: -5.4061, loss G: 105.9502\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 130/252                   Loss D: -5.4611, loss G: 104.5311\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 140/252                   Loss D: -4.3212, loss G: 106.3755\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 150/252                   Loss D: -5.4081, loss G: 113.5290\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 160/252                   Loss D: -5.0401, loss G: 106.7006\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 170/252                   Loss D: -5.2057, loss G: 107.4457\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 180/252                   Loss D: -5.4642, loss G: 108.0684\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 190/252                   Loss D: -7.3832, loss G: 108.4550\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 200/252                   Loss D: -4.3787, loss G: 108.9651\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 210/252                   Loss D: -6.8702, loss G: 106.4269\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 220/252                   Loss D: -7.0196, loss G: 103.8076\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 230/252                   Loss D: -6.0830, loss G: 106.9048\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 240/252                   Loss D: -8.6709, loss G: 106.7281\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [33/100] Batch 250/252                   Loss D: -5.1877, loss G: 106.1817\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 10/252                   Loss D: -6.3448, loss G: 108.6758\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 20/252                   Loss D: -4.9239, loss G: 107.6972\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 30/252                   Loss D: -6.3783, loss G: 109.3177\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 40/252                   Loss D: -5.3653, loss G: 104.8435\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 50/252                   Loss D: -5.7980, loss G: 105.9340\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 60/252                   Loss D: -5.2708, loss G: 107.3286\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 70/252                   Loss D: -5.0225, loss G: 113.2575\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 80/252                   Loss D: -4.7277, loss G: 107.3352\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 90/252                   Loss D: -7.1336, loss G: 115.0157\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 100/252                   Loss D: -4.3379, loss G: 108.3727\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 110/252                   Loss D: -6.0684, loss G: 110.6327\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 120/252                   Loss D: -5.4275, loss G: 103.1557\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 130/252                   Loss D: -4.9920, loss G: 107.2363\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 140/252                   Loss D: -4.8225, loss G: 105.7839\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 150/252                   Loss D: -4.8307, loss G: 107.1003\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 160/252                   Loss D: -5.1611, loss G: 108.8045\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 170/252                   Loss D: -4.1227, loss G: 107.2188\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 180/252                   Loss D: -6.2096, loss G: 111.6013\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 190/252                   Loss D: -5.3602, loss G: 111.4009\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 200/252                   Loss D: -5.5729, loss G: 106.5711\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 210/252                   Loss D: -4.7420, loss G: 106.8569\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 220/252                   Loss D: -5.5743, loss G: 110.3105\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 230/252                   Loss D: -5.6206, loss G: 108.5581\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 240/252                   Loss D: -4.4207, loss G: 103.0228\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [34/100] Batch 250/252                   Loss D: -8.1400, loss G: 105.9468\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 10/252                   Loss D: -4.9411, loss G: 100.4323\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 20/252                   Loss D: -6.4125, loss G: 105.9127\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 30/252                   Loss D: -6.4968, loss G: 103.7615\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 40/252                   Loss D: -6.2385, loss G: 111.8796\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 50/252                   Loss D: -6.3080, loss G: 109.5085\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 60/252                   Loss D: -6.2329, loss G: 109.9609\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [35/100] Batch 70/252                   Loss D: -7.2916, loss G: 111.2118\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training of WGAN-GP\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from progan import custom_loader_rgba\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 4\n",
    "NUM_CLASSES = 3\n",
    "GEN_EMBEDDING = 100\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 100\n",
    "FEATURES_CRITIC = 16\n",
    "FEATURES_GEN = 16\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "PROJECT_NAME = \"CGAN\"\n",
    "CHECKPOINT_GEN = f\"{PROJECT_NAME}/generator.pth\"\n",
    "CHECKPOINT_CRITIC = f\"{PROJECT_NAME}/critic.pth\"\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        #transforms.CenterCrop([IMAGE_SIZE, IMAGE_SIZE]),\n",
    "        transforms.Resize([IMAGE_SIZE, IMAGE_SIZE]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "# comment mnist above and uncomment below for training on CelebA\n",
    "dataset = datasets.ImageFolder(root=\"D:\\\\Study\\\\githubRepo\\\\summer-session-sw-project\\\\data\\\\rgba\", transform=transforms, loader = custom_loader_rgba)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# initialize gen and disc, note: discriminator should be called critic,\n",
    "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "# gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "# critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
    "gen = CondGenerator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMAGE_SIZE, GEN_EMBEDDING).to(device)\n",
    "critic = CondDiscriminator(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMAGE_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/{PROJECT_NAME}/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/{PROJECT_NAME}/fake\")\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (real, labels) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        # equivalent to minimizing the negative of that\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            # fake = gen(noise)\n",
    "            fake = gen(noise, labels)\n",
    "            \n",
    "            # critic_real = critic(real).reshape(-1)\n",
    "            # critic_fake = critic(fake).reshape(-1)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            # gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            gp = cond_gradient_penalty(critic, labels, real, fake, device=device)\n",
    "\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        # gen_fake = critic(fake).reshape(-1)\n",
    "        gen_fake = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 10 == 0 and batch_idx > 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # fake = gen(fixed_noise)\n",
    "                fake = gen(fixed_noise, labels[:32])\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real(cgan)\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake(cgan)\", img_grid_fake, global_step=step)\n",
    "            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
    "            save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
    "            step += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0857cb0f6eaafc429a144dff2c8ec99ab87e307df337c6e8cac39982794126bb"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
